{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Assignment\n",
    "## Cliff Walking with Reinforcement Learning\n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    ">**Make sure** you include your name along with the name of your team and team members in the notebook you submit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your name and team name here:** \n",
    "### Team Cobra\n",
    "- Zhong Gao\n",
    "- Heng Li\n",
    "- Matt Smith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this challenge you will apply Monte Carlo reinforcement learning algorithms to a classic problem in reinforcement learning, known as the **cliff walking problem**. The cliff walking problem is a type of game. The goal is for the agent to find the highest reward (lowest cost) path from a starting state to the goal.   \n",
    "\n",
    "There are a number of versions of the cliff walking problems which have been used as research benchmarks over the years. You can find a short discussion of the cliff walking problem on page 132 of Sutton and Barto, second edition.    \n",
    "\n",
    "In the general cliff walking problem the agent starts in one corner of the state-space and must travel to goal, or terminal state, in another corner of the state-space. Between the starting state and goal state there is an area with a **cliff**. If the agent falls off a cliff it is sent back to the starting state. A schematic diagram of the state-space is shown in the diagram below.      \n",
    "\n",
    "<img src=\"CliffWalkingDiagram.JPG\" alt=\"Drawing\" style=\"width:500px; height:400px\"/>\n",
    "<center> State-space of cliff-walking problem </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Description\n",
    "\n",
    "The agent must learn a policy to navigate from the starting state to the terminal state. The properties this problem are as follows:\n",
    "\n",
    "1. The state-space has two **continuous variables**, x and y.\n",
    "2. The starting state is at $x = 0.0$, $y = 0.0$. \n",
    "3. The terminal state has two segments:\n",
    "  - At $y = 0.0$ is in the range $9.0 \\le x \\le 10.0$. \n",
    "  - At $x = 10.0$ is in the range $0.0 \\le y \\le 1.0$.  \n",
    "4. The cliff zone is bounded by:\n",
    "  - $0.0 \\le y \\le 1.0$ and \n",
    "  - $1.0 \\le x \\le 9.0$. \n",
    "5. An agent entering the cliff zone is returned to the starting state.\n",
    "6. The agent moves 1.0 units per time step. \n",
    "7. The 8 possible **discrete actions** are moves in the following directions:  \n",
    "  - +x, \n",
    "  - +x, +y,\n",
    "  - +y\n",
    "  - -x, +y,\n",
    "  - -x,\n",
    "  - -x, -y,\n",
    "  - -y, and\n",
    "  - +x, -y. \n",
    "8. The rewards are:\n",
    "  - -1 for a time step in the state-space,\n",
    "  - -10 for colliding with an edge (barrier) of the state-space,\n",
    "  - -100 for falling off the cliff and returning to the starting state, and \n",
    "  - +1000 for reaching the terminal or goal state. \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "In this challenge you and your team will do the following. Include commentary on each component of your algorithms. Make sure you answer the questions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Simulator   \n",
    "\n",
    "Your reinforcement learning agent cannot contain any information about the environment other that the starting state and the possible actions. Therefore, you must create an environment simulator, with the following input and output:\n",
    "- Input: Arguments of state, the $(x,y)$ tuple, and discrete action\n",
    "- Output: the new state (s'), reward, and if the new state meets the terminal or goal criteria.\n",
    "\n",
    "Make sure you test your simulator functions carefully. The test cases must include, steps with each of the actions, falling off the cliff from each edge, hitting the barriers, and reaching the goal (terminal) edges. Errors in the simulator will make the rest of this challenge difficult.   \n",
    "\n",
    "> **Note**: For this problem, coordinate state is represented by a tuple of continuous variables. Make sure that you maintain coordinate state as continuous variables for this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: (0, 1), Reward: -1, Terminal: False\n",
      "State: (0.7071067811865476, 1.7071067811865475), Reward: -1, Terminal: False\n",
      "State: (1.7071067811865475, 1.7071067811865475), Reward: -1, Terminal: False\n",
      "State: (0, 0), Reward: -100, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 1), Reward: -1, Terminal: False\n",
      "State: (0.7071067811865476, 1.7071067811865475), Reward: -1, Terminal: False\n",
      "State: (1.7071067811865475, 1.7071067811865475), Reward: -1, Terminal: False\n",
      "State: (0, 0), Reward: -100, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 1), Reward: -1, Terminal: False\n",
      "State: (0.7071067811865476, 1.7071067811865475), Reward: -1, Terminal: False\n",
      "State: (1.7071067811865475, 1.7071067811865475), Reward: -1, Terminal: False\n",
      "State: (0, 0), Reward: -100, Terminal: False\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "from math import cos\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Variables\n",
    "n_states = 10**2\n",
    "n_episodes = 1000\n",
    "radian = (45 * math.pi)/180 #for diagonal movement\n",
    "initial_state = tuple((0,0))\n",
    "state = tuple((0,0))\n",
    "action_index = {0: tuple((0,1)), 1: tuple((1,1)), 2: tuple((1,0)), \n",
    "                3: tuple((1,-1)), 4: tuple((0,-1)), 5: tuple((-1,-1)), \n",
    "                6:tuple((-1,0)), 7: tuple((-1,-1))}\n",
    "n_actions = len(action_index)\n",
    "\n",
    "\n",
    "def sim_walk(state, action):\n",
    "    # Translate action diagonals if necessary\n",
    "    if (abs(action[0]) == abs(action[1])):\n",
    "            action = tuple(np.multiply(action, tuple((cos(radian), cos(radian)))))\n",
    "\n",
    "    # Update position/state\n",
    "    terminal = False\n",
    "    state_prime = tuple(np.add(state, action))\n",
    "    \n",
    "    # Check location in grid and terminal state\n",
    "    grid_prime = tuple((math.floor(state_prime[0]), math.floor(state_prime[1]))) # the current state in grid units\n",
    "    \n",
    "    reward = -1\n",
    "\n",
    "    # Restart if off cliff\n",
    "    if (off_cliff(state_prime)):\n",
    "        state_prime = tuple((0,0))\n",
    "        reward = -100\n",
    "        \n",
    "    # Check if goal is met (before boundary)\n",
    "    if (is_terminal(state_prime)):\n",
    "        state_prime = grid_prime\n",
    "        terminal = True\n",
    "        reward = 1000\n",
    "                       \n",
    "    # Check if boundary hit\n",
    "    if (off_grid(state_prime)):\n",
    "        state_prime = state\n",
    "        reward = -10\n",
    "        \n",
    "    return (state_prime, grid_prime, reward, terminal)\n",
    "    \n",
    "def off_cliff(current_state):\n",
    "    return ((1 <= current_state[0] <= 9) & \n",
    "            ((0 <= current_state[1] <= 1)))\n",
    "\n",
    "def is_terminal(current_state):\n",
    "    return ((current_state[0] > 9) & \n",
    "            (current_state[1] < 1))\n",
    "\n",
    "def off_grid(current_state):\n",
    "    return ((current_state[0] < 0) | \n",
    "            (current_state[0] > 10) | \n",
    "            (current_state[1] < 0) | \n",
    "            (current_state[1] > 10))\n",
    "\n",
    "## Test the function\n",
    "\n",
    "state_list = [state]\n",
    "for i in range(20):\n",
    "    s_prime, reward, terminal = sim_walk(state, action_index[i % 8])\n",
    "    state = s_prime\n",
    "    state_list.append(s_prime)\n",
    "    print(f\"State: {state}, Reward: {reward}, Terminal: {terminal}\")\n",
    "    \n",
    "def plot_walk(states):  \n",
    "    plt.axis([-0.1,10, -0.1,10])\n",
    "    plt.title(\"Cliff Walk\")\n",
    "    plt.plot(*zip(*states))\n",
    "    plt.show()\n",
    "    \n",
    "plot_walk(state_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n",
      "State: (0, 0), Reward: -10, Terminal: False\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "magnitude = 1\n",
    "def radians(degrees):\n",
    "    return (degrees * math.pi)/180\n",
    "current_state = (0,0)\n",
    "random = np.random.random() * 360\n",
    "action = tuple((math.sin(radians(random)), math.cos(radians(random))))\n",
    "\n",
    "def get_action():\n",
    "    return tuple((math.sin(radians(random)), math.cos(radians(random))))\n",
    "\n",
    "state_list = [current_state]\n",
    "for i in range(20):\n",
    "    s_prime, reward, terminal = sim_walk(current_state, get_action())\n",
    "    current_state = s_prime\n",
    "    state_list.append(s_prime)\n",
    "    print(f\"State: {current_state}, Reward: {reward}, Terminal: {terminal}\")\n",
    "    \n",
    "def plot_walk(states):  \n",
    "    plt.axis([-0.1,10, -0.1,10])\n",
    "    plt.title(\"Cliff Walk\")\n",
    "    plt.plot(*zip(*states))\n",
    "    plt.show()\n",
    "    \n",
    "plot_walk(state_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Approximation\n",
    "\n",
    "The state-space of the cliff walking problem is continuous. Therefor, you will need to use a **grid approximation** to construct a policy. The policy is specified as the probability of action for each grid cell. For this problem, use a 10x10 grid. \n",
    "\n",
    "> **Note:** While the policy uses a grid approximation, state should be represented as continuous variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Policy\n",
    "\n",
    "Start with a uniform initial policy. A uniform policy has an equal probability of taking any of the 8 possible actions for each cell in the grid representation.     \n",
    "\n",
    "> **Note:** As has already been stated, the coordinate state representation for this problem is a tuple of coordinate values. However, policy, state-values and action-values are represented with a grid approximation. \n",
    "\n",
    "> **Hint:** You may wish to use a 3-dimensional numpy array to code the policy for this problem. With 8 possible actions, this approach will be easier to work with. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125]],\n",
       "\n",
       "       [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125]],\n",
       "\n",
       "       [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125]],\n",
       "\n",
       "       [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125]],\n",
       "\n",
       "       [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125]],\n",
       "\n",
       "       [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125]],\n",
       "\n",
       "       [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125]],\n",
       "\n",
       "       [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125],\n",
       "        [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,\n",
       "         0.125]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_policy = np.ones((8, 10, 10)) / 8\n",
    "initial_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo State Value Estimation   \n",
    "\n",
    "For the initial uniform policy, compute the state values using the Monte Carlo RL algorithm:\n",
    "1. Compute and print the state values for each grid in the representation. Use at least 1,000 episodes. This will take some time to execute.      \n",
    "2. Plot the grid of state values, as an image (e.g. matplotlib [imshow](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.imshow.html)). \n",
    "3. Compute the Forbenious norm (Euclidean norm) of the state value array with [numpy.linalg.norm](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html). You will use this figure as a basis to compare your improved policy. \n",
    "\n",
    "Study your plot to ensure your state values seem correct. Do these state values seem reasonable given the uniform policy and why? Make sure you pay attention to the state values of the cliff zone.    \n",
    "\n",
    "> **Hint:** Careful testing at each stage of your algorithm development will potentially save you considerable time. Test your function(s) to for a single episode to make sure your algorithm converges. Then test for say 10 episodes to ensure the state values update in a reasonable manner at each episode.    \n",
    "\n",
    "> **Note:** The Monte Carlo episodes can be executed in parallel for production systems. The Markov chain of each episode is statistically independent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1), (0.7071067811865476, 0.7071067811865476), -1, False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def take_action(state, policy, actions = action_index): \n",
    "    action = actions[nr.choice(range(len(actions)), p = policy[:, state[0], state[1]])]\n",
    "    s_prime, reward, is_terminal = sim_walk(state, action)\n",
    "    return (action, s_prime, reward, is_terminal)\n",
    "\n",
    "take_action((0,0), initial_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_episode(policy, G, n_visits, episode, n_states):\n",
    "    ## For each episode we use a list to keep track of states we have visited.\n",
    "    ## Once we visit a state we need to accumulate values to get the returns\n",
    "    states_visited = []\n",
    "    states = list(range(0,policy.shape[1:][0] * policy.shape[1:][1]))\n",
    "        \n",
    "    \n",
    "    ## Find the starting state\n",
    "    current_state = tuple((0,0))\n",
    "    terminal = False\n",
    "    g = 0.0\n",
    "        \n",
    "    while(not terminal):\n",
    "        ## Find the next action and reward\n",
    "        action, s_prime, reward, terminal = take_action(current_state, policy)\n",
    "        \n",
    "        ## Add the reward to the states visited if this is a first visit  \n",
    "        if(current_state not in states_visited):\n",
    "            ## Mark that the current state has been visited \n",
    "            states_visited.append(current_state) \n",
    "            ## Add the reward to states visited \n",
    "            for state in states_visited:  \n",
    "                \n",
    "                n_visits[state[0]][state[1]] = n_visits[state[0]][state[1]] + 1.0\n",
    "                G[state[0]][state[1]] = G[state[0]][state[1]] + (reward - G[state[0]][state[1]])/n_visits[state[0]][state[1]]\n",
    "\n",
    "        ## Update the current state for next transition\n",
    "        current_state = find_grid_state(s_prime)\n",
    "    return (G, n_visits) \n",
    "    \n",
    "    \n",
    "def MC_state_values(policy, n_episodes):\n",
    "    ## Create list of states \n",
    "    states = list(range(0,policy.shape[1:][0] * policy.shape[1:][1]))\n",
    "    n_states = len(states)\n",
    "    \n",
    "    ## An array to hold the accumulated returns as we visit states\n",
    "    G = np.zeros(policy.shape[1:])\n",
    "    \n",
    "    ## An array to keep track of how many times we visit each state so we can \n",
    "    ## compute the mean\n",
    "    n_visits = np.zeros(policy.shape[1:])\n",
    "    \n",
    "    ## Iterate over the episodes\n",
    "    for i in range(n_episodes):\n",
    "        G, n_visits = MC_episode(policy, G, n_visits, i, n_states)\n",
    "    \n",
    "    return(G) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 2 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-68c32fd5a32a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstate_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMC_state_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-0468a6dd2fc2>\u001b[0m in \u001b[0;36mMC_state_values\u001b[0;34m(policy, n_episodes)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_visits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMC_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_visits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_states\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# neighbors, i, n_states)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-0468a6dd2fc2>\u001b[0m in \u001b[0;36mMC_episode\u001b[0;34m(policy, G, n_visits, episode, n_states)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m## Find the next action and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m## Add the reward to the states visited if this is a first visit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b8877bac2af5>\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(state, policy, actions)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0ms_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_terminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim_walk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_terminal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 10 is out of bounds for axis 2 with size 10"
     ]
    }
   ],
   "source": [
    "nr.seed(234)\n",
    "state_values = MC_state_values(initial_policy, n_episodes = 5000)\n",
    "print(state_values.reshape((10,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo State Policy Improvement   \n",
    "\n",
    "Finally, you will perform Monte Carlo RL policy improvement:\n",
    "1. Starting with the uniform policy, compute action-values for each grid in the representation. Use at least 1,000 episodes.      \n",
    "2. Use these action values to find an improved policy.\n",
    "3. To evaluate your updated policy compute the state-values for this policy.  \n",
    "4. Plot the grid of state values for the improved policy, as an image. \n",
    "5. Compute the Forbenious norm (Euclidean norm) of the state value array. \n",
    "\n",
    "Compare the state value plot for the improved policy to the one for the initial uniform policy. Does the improved state values increase generally as distance to the terminal states decreases?  Is this what you expect and why?    \n",
    "\n",
    "Compare the norm of the state values with your improved policy to the norm for the uniform policy. Is the increase significant?  \n",
    "\n",
    "> **Hint:** Careful testing at each stage of your algorithm development will potentially save you considerable time. Test your function(s) to for a single episode to make sure your algorithm converges. Then test for say 10 episodes to ensure the state values update in a reasonable manner at each episode.   \n",
    "\n",
    "> **Note:** You could continue to improve policy using the general policy improvement algorithm (GPI). In the interest of time, you are not required to do so here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_Q(Q):\n",
    "    Q = pd.DataFrame(Q, columns = ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW'])\n",
    "    print(Q)\n",
    "\n",
    "def MC_action_values(policy, Q, n_episodes, inital_state):\n",
    "    n_states = len(policy)\n",
    "    n_actions = len(policy[0])\n",
    "    n_visits = np.zeros((n_states, n_actions))\n",
    "    neighbors = {}\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        Q, n_visits = MC_action_value_episode(policy, Q, n_visits, initial_state, n_states, n_actions)\n",
    "    return(Q)\n",
    "\n",
    "initial_Q = np.zeros((n_states, n_actions))\n",
    "updated_Q = MC_action_values(initial_policy, initial_Q, n_episodes, initial_state)\n",
    "print_Q(updated_Q)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_copy = deepcopy(initial_policy)\n",
    "\n",
    "def update_policy(policy, Q, epsilon, action_index = action_index):\n",
    "    keys = list(policy[0].keys())\n",
    "    \n",
    "    for state in range(len(policy)): \n",
    "        q = Q[state,:]\n",
    "        max_action_index = np.where(q == max(q))[0]\n",
    "\n",
    "        n_transitions = float(len(q))\n",
    "        n_max_transitions = float(len(max_action_index))\n",
    "        p_max_transitions = (1.0 - epsilon *(n_transitions - n_max_transitions))/(n_max_transitions)\n",
    "  \n",
    "        for key in keys:\n",
    "            if(action_index[key] in max_action_index): policy[state][key] = p_max_transitions\n",
    "            else: policy[state][key] = epsilon\n",
    "    return(policy)                \n",
    "\n",
    "improved_policy = update_policy(initial_copy, initial_Q, 0.01) \n",
    "improved_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(457)\n",
    "state_values = MC_state_values(improved_policy, n_episodes = 10000)\n",
    "print(state_values.reshape((10,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Create cells below for your solution to the stated problem. Be sure to include some Markdown text and code comments to explain each component of your algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
